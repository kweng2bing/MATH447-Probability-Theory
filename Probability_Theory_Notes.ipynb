{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNm4lIsmmwqTPHRckt8NLjN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kweng2bing/MATH447-Probability-Theory/blob/main/Probability_Theory_Notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter 2- Probability\n",
        "* Probability of an event\n",
        "* Counting Sample Points\n",
        "* Conditional Probability & Independence\n",
        "* Laws of Probability\n",
        "* Law of Total Probability & Bayes Rule\n",
        "* Random Sampling\n",
        "* Miscellaneous"
      ],
      "metadata": {
        "id": "56uH4yhxQ6UV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Probability\n",
        "\n",
        "A *simple event* is an event that cannot be decomposed. Each simple event\n",
        "corresponds to one and only one *sample point*. The letter $E$ with a subscript\n",
        "will be used to denote a simple event or the corresponding sample point.\n",
        "\n",
        "The *sample space* associated with an experiment is the set consisting of all possible sample points. A sample space will be denoted by $S$.\n",
        "\n",
        "\n",
        "Suppose $S$ is a sample space associated with an experiment. To every event $A$\n",
        "in $S$ ($A$ is a subset of $S$), we assign a number, $P(A)$, called the probability of\n",
        "$A$, so that the following axioms hold:\n",
        "\n",
        "Axiom 1: $P(A) ≥ 0$\n",
        "\n",
        "Axiom 2: $P(S) = 1$\n",
        "\n",
        "Axiom 3: If $A_{1}, A_{2}, A_{3},...$ form a sequence of pairwise mutually\n",
        "exclusive events in $S$ (that is, $A_{i} ∩ A_{j} = ∅$ if $i \\neq j$), then\n",
        "\n",
        "$$P(A_{1} \\cup A_{2} \\cup A_{3} \\cup ···) = \\sum_{i=1}^{\\infty}P(A_{i})$$"
      ],
      "metadata": {
        "id": "dHpFxwHa-nz8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Counting Sample Points\n",
        "\n",
        "With $m$ elements $a_{1}, a_{2},..., a_{m}$ and $n$ elements $b_{1}, b_{2},..., b_{n}$, it is possible to form $mn = m × n$ pairs containing one element from each group.\n",
        "\n",
        "An ordered arrangement ofr distinct objects is called a *permutation*. The number of ways of ordering $n$ distinct objects taken $r$ at a time will be designated by the symbol $P^{n}_{r}$.\n",
        "\n",
        "$$P^{n}_{r} = n(n-1)(n-2)... (n-r+1) = \\frac{n!}{n-r!}$$\n",
        "\n",
        "<br>\n",
        "\n",
        "The number of ways of partitioning n distinct objects into $k$ distinct groups\n",
        "containing $n_{1}, n_{2},..., n_{k}$ objects, respectively, where each object appears in exactly one group and $\\sum_{i=1}^{k}n_{i}= n$, is\n",
        "$$ N = \\binom{n}{n_{1} n_{2} ··· n_{k}}= \\frac{n!}{n_{1}! n_{2}! ··· n_{k}!}$$\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "The number of combinations of n objects taken $r$ at a time is the number of\n",
        "subsets, each of size $r$, that can be formed from the n objects. This number will\n",
        "be denoted by $C^{n}_{r}$ or $\\binom{n}{r}$.\n",
        "\n",
        "The number of unordered subsets of size $r$ chosen (without replacement) from $n$ available objects is\n",
        "$$\\binom{n}{r}= C_{n}^{r} = \\frac{P^{n}_{r}}{r!} = \\frac{n!}{r!(n − r)!}$$\n"
      ],
      "metadata": {
        "id": "bug8u-QSuSnj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Conditional Probability & Independence\n",
        "\n",
        "The *conditional probability of an event* A, given that an event B has occurred,\n",
        "is equal to\n",
        "\n",
        "$$P(A|B) =  \\begin{cases}\n",
        "\\frac{P(A ∩ B)}{P(B)} & \\text{ if } P(B) > 0. \\\\\n",
        "\\text{undefined } & \\text{ if } P(B) = 0\n",
        "\\end{cases}\n",
        "$$\n",
        "Read as : “probability of A given B.”\n",
        "\n",
        "<br>\n",
        "\n",
        "Two events $A$ and $B$ are said to be independent if any one of the following holds:\n",
        "\n",
        "$$P(A|B) = P(A)$$\n",
        "$$P(B|A) = P(B)$$\n",
        "$$P(A ∩ B) = P(A)P(B)$$\n",
        "Otherwise, the events are said to be dependent."
      ],
      "metadata": {
        "id": "qPUA9uLDtmTr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "W7jKiWY_R3Rk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Laws of Probability\n",
        "\n",
        "###Multiplicative Law of Probability\n",
        "---\n",
        "The probability of the intersection of\n",
        "two events $A$ and $B$ is\n",
        "\n",
        "$$P(A ∩ B) = P(A)P(B|A) = P(B)P(A|B)$$\n",
        "\n",
        "If $A$ and $B$ are independent, then\n",
        "$$P(A ∩ B) = P(A)P(B)$$\n",
        "\n",
        "<br>\n",
        "\n",
        "###Additive Law of of Probability\n",
        "---\n",
        "The probability of the union of two events\n",
        "$A$ and $B$ is\n",
        "\n",
        "$$P(A ∪ B) = P(A) + P(B) − P(A ∩ B)$$\n",
        "\n",
        "\n",
        "###Others\n",
        "\n",
        "If $A$ is an event, then\n",
        "$$P(A) = 1 − P(A)$$\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s6lnnKslst3U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Law of Total Probability & Bayes Rule\n"
      ],
      "metadata": {
        "id": "ndIzNBiDqWyn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For some positive integer $k$, let the sets $B_{1}, B_{2},..., B_{k}$ be such that\n",
        "\n",
        "1. $$S = B_{1} \\cup B_{2} ∪···∪ B_{k}$$\n",
        "2. $$B_{i} ∩ B_{j} = ∅ \\quad \\text{for } i \\neq j$$\n",
        "\n",
        "Then the collection of sets $ \\{B_{1}, B_{2},..., B_{k} \\}$ is said to be a partition of $S$.\n",
        "\n",
        "<br>\n",
        "\n",
        "###Law of Total Probability\n",
        "Assume that $\\{ B_{1}, B_{2},..., B_{k} \\}$ is a partition of $$ (see Definition 2.11) such that $P(B_{i}) > 0$, for $i = 1, 2,..., k$. Then for any event $A$\n",
        "\n",
        "$$P(A) = \\sum\\limits_{i=1}^{k} P(A \\vert B_{i}) P(B_{i})$$\n",
        "\n",
        "\n",
        "###Bayes' Rule\n",
        "\n",
        "Assume that $\\{B_{1}, B_{2},..., B_{k} \\}$ is a partition of $S$ (see Definition\n",
        "2.11) such that $P(B_{i}) > 0$, for $i = 1, 2,..., k$. Then\n",
        "$$P(B_{j}|A) = \\frac{P(A|B_{j})P(B_{j})}{ \\sum\\limits_{i = 1}^{k}  P(A|B_{i})P(B_{i})}$$\n"
      ],
      "metadata": {
        "id": "loG-CeDfqSZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Random Sampling\n",
        "\n",
        "A *random variable* is a real-valued function for which the domain is a sample space.\n",
        "\n",
        "Let $N$ and $n$ represent the numbers of elements in the population and sample,\n",
        "respectively. If the sampling is conducted in such a way that each of the $\\binom{N}{n}$ samples has an equal probability of being selected, the sampling is said to be\n",
        "random, and the result is said to be a *random sample*."
      ],
      "metadata": {
        "id": "p3DVuw1wpiJ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Miscellaneous"
      ],
      "metadata": {
        "id": "7XLQiYAAQIMl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "### $\\sigma$-algebra\n",
        "---\n",
        "Let $Ω$ be a nonempty set and $\\mathfrak{F} ⊆ 2^{Ω}$ a collection of subsets of Ω, such\n",
        "that\n",
        "*  $A ∈ \\mathfrak{F} ⇒ A^{\\complement}∈ F$\n",
        "* $A_{n} ∈ \\mathfrak{F}$ arbitrary $⇒ \\bigcup\\limits_{j=1}^{∞} A_{j} ∈ \\mathfrak{F}$\n",
        "* $∅ ∈ F$\n",
        "\n",
        "Then we call F a σ-algebra.\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "* The triplet $(Ω, \\mathfrak{F}, P)$ is called a probability space.\n",
        "\n",
        "\n",
        "* We call $\\mathfrak{B}$ and $\\mathfrak{B}^{n}$ the Borel $σ$-algebras of $\\mathfrak{R}$ and of \\mathfrak{R}^{n} Borel σ-algebra and we call their members Borel sets."
      ],
      "metadata": {
        "id": "Ctb_jcUIN5QQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are $\\binom{n-1}{r-1}$ distinct integer valued vectors $\\vec{x} = (x_{1}, x_{2}, ..., x_{r})$ s.t. $$x_{1} + x_{2} + ...+ x_{r} = n \\quad \\text{and} \\quad x_{i} > 0, i = 1,...r $$\n",
        "\n",
        "* $\\binom{n-1}{r-1}$ ways to select $n$ indistinguishable items into $r$ distinct bins s.t. each bin contains at least one\n",
        "\n",
        "There are $\\binom{n+r-1}{r-1}$ distinct integer valued vectors $\\vec{y} = (y_{1}, y_{2}, ..., y_{r})$ s.t. $$y_{1} + y_{2} + ...+ y_{r} = n \\quad \\text{and} \\quad y_{i} \\geq 0, i = 1,...r $$\n",
        "* $\\binom{n+r-1}{r-1}$ ways to select $n$ indistinguishable items into $r$ distinct bins"
      ],
      "metadata": {
        "id": "i0MkhC7KTjpn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Bernoulli & Independent and Identically Distributed(iid)\n",
        "\n",
        "Bernoulli random item - a random item which only assumes two outcomes\n",
        "* $S$ (sucess) or $F$ (failure)\n",
        "* 1 or 0\n",
        "\n",
        "Independent and Identically Distributed(iid)\n",
        "\n",
        "Let $X_{1}, X_{2}, . . .(Ω, P) → Ω'$ be a sequence of random items\n",
        "* $X_{j}$ are independent\n",
        "* Same Distribution\n",
        " * $P_{X_{i}} = P_{X_{j}} $"
      ],
      "metadata": {
        "id": "A3FJE2pvVlxZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter 3- Discrete\n",
        "* Introduction\n",
        "  * Probability Distribution\n",
        "  * Expected Value\n",
        "* Binomial Distribution\n",
        "* Geometric Distribution\n",
        "* Negative Binomial Distribution\n",
        "* Hypergeometric Distribution\n",
        "* Poisson Distribution\n",
        "* Moment Generating Functions\n",
        "* Probability Generating Functions\n",
        "* Tchebysheff's Theorem"
      ],
      "metadata": {
        "id": "Mu0aZX-pboUK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Introduction\n",
        "---"
      ],
      "metadata": {
        "id": "wK6KCo2vckgK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Probability Distribution\n",
        "\n",
        "A random variable $Y$ is said to be *discrete* if it can assume only a finite or countably infinite number of distinct values.\n",
        "\n",
        "The probability that $Y$ takes on the value $y$, $P(Y = y)$, is defined as the *sum of the probabilities of all sample points in S* that are assigned the value $y$. We will sometimes denote $P(Y = y)$ by $p(y)$.\n",
        "\n",
        "The probability distribution for a discrete variable $Y$ can be represented by a formula, a table, or a graph that provides $p(y) = P(Y = y)$ for all $y$.\n",
        "\n",
        "For any discrete probability distribution, the following must be true:\n",
        "1. $0 ≤ p(y) ≤ 1$ for all $y$.\n",
        "2. $\\sum_{y} p(y) = 1$, where the summation is over all values of $y$ with nonzero probability.\n",
        "\n"
      ],
      "metadata": {
        "id": "h-KIbHDScnzV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Expected Values\n",
        "\n",
        "Let $Y$ be a discrete random variable with the probability function $p(y)$. Then the expected value of $Y, E(Y)$, is defined to be\n",
        " $$E(Y) = \\sum_{y}yp(y)$$\n",
        "\n",
        " Note: Expected value exists if $E(Y) = \\sum_{y}\\lvert y \\rvert p(y) < \\infty$\n",
        "\n",
        "<br>\n",
        "\n",
        " If $g(Y)$ is a real-valued function of $Y$. Then the expected value of $g(Y)$ is\n",
        " $$E[g(Y)] = \\sum_{y} g(y)p(y)$$\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "Let $Y$ be a discrete random variable with probability function $p(y)$, $c$ be a constant, $g(Y)$ be a function of $Y$. Let $g_{1}(Y), g_{2}(Y), . . . , g_{k}(Y)$ be $k$ functions of $Y$ Then\n",
        "\n",
        "1. $E(c) = c$\n",
        "2. $E[cg(Y)] = cE[g(Y)]$\n",
        "3. $E[g_{1}(Y) + g_{2}(Y) +···+ g_{k}(Y)]= E[g_{1}(Y)] + E[g_{2}(Y)] +···+ E[g_{k}(Y)]$\n",
        "4. $E \\bigg[ \\sum\\limits_{j=1}^{n} Y_{j} \\bigg] = \\sum\\limits_{j=1}^{n} E[Y_{j}]$\n",
        "\n"
      ],
      "metadata": {
        "id": "H05-nxvNdnpd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Variance\n",
        "\n",
        " If $Y$ is a discrete random variable with mean $E(Y) = μ$, the variance of a random variable $Y$ is defined to be the expected value of $(Y - μ)^{2}$. That is,\n",
        "\n",
        "$$V(Y ) = \\sigma^{2} = E[(Y − μ)^{2}] = E(Y^{2}) - \\mu^{2}$$\n",
        "\n",
        "<br>\n",
        "\n",
        "The standard deviation of $Y$ is the positive square root of $V(Y)$.\n",
        "$$\\sigma = \\sqrt{\\sigma^{2}}$$\n",
        "\n",
        "<br>\n",
        "\n",
        "Let $Y$ be a discrete random variable and $a, b ∈ R$. Then\n",
        "$$V[aY + b] = a^{2}V[Y]$$\n",
        "\n",
        "<br>\n",
        "\n",
        "####Bienaymé formula\n",
        "\n",
        "Let $Y_{1}, Y_{2}, . . . , Y_{n} : Ω → \\mathbb{R}$ be **independent** discrete random variables\n",
        "$$V\\bigg[\\sum\\limits_{j=1}^{n} Y_{j}\\bigg] = \\sum_{j=1}^{n} V[Y_{j}]$$"
      ],
      "metadata": {
        "id": "Vj-ts8aDU5SE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Binomial Distribution\n",
        "\n",
        "\n",
        "\n",
        "A *binomial experiment* possesses the following properties:\n",
        "1. The experiment consists of a fixed number, $n$, of identical trials.\n",
        "2. Each trial results in one of two outcomes: success, $S$, or failure, $F$.\n",
        "3. The probability of success on a single trial is equal to some value $p$ and remains the same from trial to trial. The probability of a failure is equal to $q = (1 - p)$.\n",
        "4. The trials are independent.\n",
        "5. The random variable of interest is $Y$, the number of successes observed during the $n$ trials.\n",
        "\n",
        "<br>\n",
        "\n",
        "A random variable $Y$ is said to have a binomial distribution, binom($n, p$), based on $n$ trials with success probability $p$ iff\n",
        "\n",
        "$$p(y) = \\binom{n}{y} p^{y}q^{n-y} \\quad y = 0, 1, 2,..., n \\text{ and } 0 \\leq p \\leq 1.$$\n",
        "\n",
        "Also,\n",
        "* $$ \\mu = E(Y) = np$$\n",
        "* $$ σ^{2} = V(Y) = npq$$\n",
        "* $$m(t) = pe^{t} + (1-p)^{n}$$"
      ],
      "metadata": {
        "id": "0RazWeoAgj_l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Geometric Distribution\n",
        "A random variable $Y$ is said to have a geometric probability distribution, geom($p$), iff\n",
        "\n",
        "$$p(y) = q^{y-1}p \\quad y = 1, 2, 3,..., 0 \\leq p \\leq 1$$\n",
        "\n",
        "Also,\n",
        "* $$μ = E(Y ) = \\frac{1}{p} $$\n",
        "* $$σ^{2} = V(Y) = \\frac{1 - p}{p^{2}} $$\n",
        "* $$m(t) = \\frac{pe^{t}}{1-(1-p)e^{t}}$$"
      ],
      "metadata": {
        "id": "DjKIK1VYR4D9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Negative Binomial Distribution\n",
        "A random variable $Y$ is said to have a negative binomial probability distribution iff\n",
        "$$ p(y) = \\binom{y - 1}{r - 1} p^{r}q^{y-r} \\quad y = r, r + 1, r + 2,..., 0 \\leq p \\leq 1$$\n",
        "\n",
        "Also\n",
        "* $$μ = E(Y ) = \\frac{r}{p}$$\n",
        "* $$σ^{2} = V(Y ) = \\frac{r(1 − p)}{p^{2}}$$\n",
        "* $$m(t) = \\Big[\\frac{pe^{t}}{1-(1-p)e^{t}} \\Big]^{r}$$"
      ],
      "metadata": {
        "id": "LmINCzZeSb-k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hypergeometric Distribution\n",
        "A random variable $Y$ is said to have a hypergeometric probability distribution iff\n",
        "$$ p(y) = \\frac{\\binom{r}{y} \\binom{N-r}{n-y}}{\\binom{N}{n}}$$\n",
        "where $y$ is an integer $0, 1, 2,..., n$, subject to the restrictions $y \\leq r$ and\n",
        "$n - y \\leq N - r$.\n",
        "\n",
        "Also,\n",
        "\n",
        "$$μ = E(Y) = \\frac{nr}{N}$$\n",
        "\n",
        "<center> and </center>\n",
        "\n",
        "$$ σ^{2} = V(Y) = n \\bigg(\\frac{r}{N} \\bigg)\\bigg(\\frac{N - r}{N} \\bigg) \\bigg( \\frac{N - n}{N - 1} \\bigg)$$\n"
      ],
      "metadata": {
        "id": "orIFd1cswCaG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Poisson distribution\n",
        "A random variable $Y$ is said to have a Poisson probability distribution, poisson($λ$), iff\n",
        "\n",
        "$$ p(y) = \\frac{λ^{y}}{y!}e^{−λ} \\quad  y = 0, 1, 2,..., λ > 0$$\n",
        "\n",
        "Also\n",
        "* $$\\mu = E[Y] = \\lambda$$\n",
        "* $$\\sigma^{2} = V[Y] = \\lambda$$\n",
        "* $$m(t) = e^{\\lambda (e^{t} - 1)}$$"
      ],
      "metadata": {
        "id": "ADvovjrZxbLz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Moment Generating functions\n",
        "\n",
        "The $k$th moment of a random variable $Y$ taken about the origin is defined to be\n",
        "$E(Y^{k})$ and is denoted by $μ_{k}'$.\n",
        "\n",
        "<br>\n",
        "\n",
        "The $k$th moment of a random variable $Y$ taken about its mean, or the $k$th central moment of $Y$, is defined to be $E[(Y − μ)^{k} ]$ and is denoted by $μ_{k}$.\n",
        "\n",
        "<br>\n",
        "\n",
        "The moment-generating function $m(t)$ for a random variable $Y$ is defined to be\n",
        "$m(t) = E(e^{tY})$. We say that a moment-generating function for $Y$ exists if $∃$ a positive constant $b$ such that $m(t)$ is finite for $|t| \\leq b$.\n",
        "\n",
        "<br>\n",
        "\n",
        "If $m(t)$ exists, then for any positive integer $k$,\n",
        "$$\\frac{d^{t}m(t)}{dt^{k}}\\bigg\\vert_{t=0} = m^{(k)}(0) = μ_{k}$$\n",
        "\n",
        "In other words, if you find the kth derivative of $m(t)$ with respect to $t$ and\n",
        "then set $t = 0$, the result will be $μ_{k}$'"
      ],
      "metadata": {
        "id": "-cbOL8zD1cRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Probability-Generating Functions\n",
        "\n",
        "Let $Y$ be an integer-valued random variable for which $P(Y = i) = p_{i}$, where\n",
        "$i = 0, 1, 2,...$. The *probability-generating function* $P(t)$ for $Y$ is defined to be\n",
        "\n",
        "$$P(t) = E(t^{Y}) = p_{0} + p_{1}t + p_{2}t\n",
        "^{2} +···= \\sum\\limits_{i=0}^{∞}p_{i} t^{i} \\quad \\forall t \\text{ s.t. } P(t) \\text{is finite}$$\n",
        "\n",
        "<br>\n",
        "\n",
        "The $k$th factorial moment for a random variable $Y$ is defined to be\n",
        "$$μ_{[k]} = E[Y (Y − 1)(Y − 2)···(Y − k + 1)]  \\quad \\text{ where } k \\in \\mathbb{Z^{+}}$$.\n",
        "\n",
        "<br>\n",
        "\n",
        "If $P(t)$ is the probability-generating function for an integer-valued random variable,$Y$, then the $k$th factorial moment of $Y$ is given by\n",
        "$$\\frac{d^{k} P(t)}{dt^{k}}\\bigg\\vert_{t=1} = P^{k}(1) = μ_{[k]}$$"
      ],
      "metadata": {
        "id": "24pnrYimBqRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tchebysheff's Theorem\n",
        "\n",
        " Let $Y$ be a random variable with mean μ and finite variance $σ^{2}$. Then, for any constant $k > 0$,\n",
        "$$ P( \\lvert Y - μ \\rvert < k \\sigma) \\geq 1 - \\frac{1}{k^{2}}$$\n",
        "<center>  or </center>\n",
        "\n",
        "$$P(|Y - μ| \\geq kσ) \\leq \\frac{1}{k^{2}}$$"
      ],
      "metadata": {
        "id": "5yc1hL86i-LQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter 4- Continuous\n",
        "* Introduction\n",
        " * Probability Distribution\n",
        " * Probability Density\n",
        " * Quantile\n",
        " * Expected Values\n",
        "* Uniform Distribution\n",
        "* Normal Distribution\n",
        "* Gamma Distribution\n",
        "* Exponenital Distribution\n",
        "* Chi-square Distribution\n",
        "* Beta Distribution\n",
        "* Moment Generating Functions\n",
        "* Tchebyseff's Theorem\n"
      ],
      "metadata": {
        "id": "dF4WN1R7cTIY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Introduction"
      ],
      "metadata": {
        "id": "1e77JNP4GdYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Probability Distribution\n",
        "---\n",
        "Let $Y$ denote any random variable. The *distribution function* of $Y$, denoted by $F(y)$, is such that $F(y) = P(Y ≤ y) $ for $−∞ < y < ∞.$\n",
        "\n",
        "####Properties\n",
        "---\n",
        "\n",
        "1. $F(-\\infty) \\equiv \\lim\\limits_{y \\rightarrow -\\infty} F(y) =0$\n",
        "2. $F(\\infty) \\equiv \\lim\\limits_{y \\rightarrow \\infty} F(y) = 1$\n",
        "3. $\\forall y_{1}, y_{2}$ if $y_{1} < y_{2}$, then $F(y_{1}) \\leq F(y_{2})$.\n",
        " * $F$ is nondecreasing\n",
        "4. $\\lim\\limits_{n→∞}F(y_{n}) = F(y).$\n",
        " * $F$ is right continuous\n"
      ],
      "metadata": {
        "id": "w-90dxoB9QLr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Probability Density\n",
        "---\n",
        " Let $F(y)$ be the distribution function for a continuous random variable $Y$. Then wherever the derivative exists, the *probability density function* for the random variable $Y$,$f(y)$, is defined as\n",
        "$$f(y) = \\frac{dF(y)}{dy} = F'(y)$$\n",
        "\n",
        "\n",
        "####Properties\n",
        "---\n",
        "1. $f(y) \\geq 0 \\quad \\forall y \\in (-\\infty, \\infty)$\n",
        "2. $\\int\\limits_{−\\infty}^{\\infty}f(y) \\,dy = 1$.\n",
        "3.  If $a, b \\in \\mathbb{R}$ and $a < b$, then\n",
        "$$ P(a \\leq Y \\leq b) = \\int_{a}^{b} f(y) \\,dy = F_{Y} (b) - F_{Y}(a)$$"
      ],
      "metadata": {
        "id": "CkrsvVhGGQR8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Quantile\n",
        "\n",
        "Let $Y$ denote any random variable and $0 < p < 1$. The $p$th *quantile* of $Y$,\n",
        "denoted by $\\phi_{p}$, is the smallest value s.t.\n",
        "$$ \\phi_{p} = \\min\\{ \\alpha \\in \\mathbb{R} : F(\\alpha) \\geq p \\} $$\n",
        "\n",
        "If $Y$ is continuous, $\\phi_{p}$ is the smallest value such that $F(\\phi_{p}) = P(Y \\leq \\phi_{p}) = p$\n",
        "\n",
        "Note\n",
        "* First quantile = $\\phi_{0.25}$\n",
        "* Median = $\\phi_{0.5}$\n",
        "* Third Quantile = $\\phi_{0.75}$\n"
      ],
      "metadata": {
        "id": "0cka-UmBHAjA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Expected Values\n",
        "---\n",
        "The expected value of a continuous random variable Y is\n",
        "$$E(Y) = \\int_{-\\infty}^{\\infty}yf(y) \\,dy$$\n",
        "\n",
        "<br>\n",
        "\n",
        "Let $g(Y)$ be a function of $Y$; then the expected value of $g(Y)$ is given by\n",
        "$$E [g(Y )] = \\int_{-\\infty}^{\\infty}g(y)f(y) \\,dy$$\n",
        "\n",
        "<br>\n",
        "\n",
        "Let $c$ be a constant and let $g(Y), g_{1}(y), g_{2}(Y), ...,  g_{k}(Y)$ be functions of a continuous random variable $Y$. Then ...\n",
        "1. $E[c] = c$\n",
        "2. $E[cg(Y)] = cE[g(Y)]$\n",
        "3. $E[g_{1}(Y) + g_{2}(Y) + ... + g_{n}(Y)]=  E[g_{1}(Y)] + E[g_{2}(Y)]+ ... + E[g_{k}(Y)]$\n",
        "4. $E \\bigg[ \\sum\\limits_{j=1}^{n} Y_{j} \\bigg] = \\sum\\limits_{j=1}^{n} E[Y_{j}]$\n",
        "\n"
      ],
      "metadata": {
        "id": "7SYLJclzGc2G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Variance\n",
        "\n",
        " If $Y$ is a continuous random variable with mean $E(Y) = μ$. Let $Y_{1}, Y_{2}, ..., Y_{n}$ be independent random variables. Let $a,b \\in \\mathbb{R}$ Then\n",
        "\n",
        "1. $V(Y) = \\sigma^{2} = E[(Y − μ)^{2}] = E(Y^{2}) - \\mu^{2}$\n",
        "2. $V[aY + b] = a^{2}V[Y]$\n",
        "3.  $V\\bigg[\\sum\\limits_{j=1}^{n} Y_{j}\\bigg] = \\sum\\limits_{j=1}^{n} V[Y_{j}]$"
      ],
      "metadata": {
        "id": "je7sO9GAgPgK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Uniform Distribution\n",
        "\n",
        "If $\\theta_{1} < \\theta_{2}$, a random variable $Y$ is said to have a continuous *uniform probability distribution*, uniform($θ_{1}, \\theta_{2}$),  on the interval $(\\theta_{1}, \\theta_{2})$ iff the density function of $Y$ is\n",
        "\n",
        "$$f(y) =\\begin{cases}\n",
        "      \\frac{1}{\\theta_{2} - \\theta_{1}} &  \\theta_{1}\\leq y \\leq \\theta_{2} \\\\\n",
        "      0 &  \\text{elsewhere} \\\\  \n",
        "   \\end{cases}$$\n",
        "\n",
        "\n",
        "Furthermore,\n",
        "$$\\mu = E[Y] = \\frac{\\theta_{1} + \\theta_{2}}{2}$$\n",
        "\n",
        "$$\\sigma^{2} = V(Y) = \\frac{(\\theta_{2} - \\theta_{1})^{2}}{12}$$"
      ],
      "metadata": {
        "id": "bUDznRshSI9S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Normal Distribution\n",
        "\n",
        "A random variable $Y$ is said to have a *normal probability distribution*, $\\mathcal{N}(\\mu, \\sigma^{2})$ iff for $\\sigma > 0$, and $-\\infty < \\mu < \\infty$ the density function of $Y$ is\n",
        "\n",
        "$$f(y) =  \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(y - \\mu)^{2}}{2\\sigma^{2}}}\\quad -\\infty < y < \\infty $$\n",
        "\n",
        "\n",
        "Furthermore,\n",
        "$$E[Y] =\\mu$$\n",
        "\n",
        "$$V(Y) = \\sigma^{2}$$\n",
        "\n",
        "$$m(t) = e^{\\mu t + \\frac{\\sigma^{2} t^{2}}{2}}$$\n",
        "\n",
        "Let $z$ be the distance from the mean of a normal distrubution expressed in unites of standard deeviation be defined as $$z =\\frac{y - \\mu}{\\sigma}$$\n",
        "\n",
        "####Emperical Rules\n",
        "* 68% of the data falls within $\\mu \\pm \\sigma$\n",
        "* 95% of the data falls within $\\mu \\pm 2\\sigma$\n",
        "* 99.7% of the data falls within $\\mu \\pm 3\\sigma$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TgKPHTyYTuBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pnorm(0.5,0,1)\n",
        "#R: `pnorm(y0,μ,σ)`"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "h16rch-lhjwn",
        "outputId": "b6b684fc-4d5c-4ff5-97b9-2e9e1f8ef359"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "0.691462461274013"
            ],
            "text/markdown": "0.691462461274013",
            "text/latex": "0.691462461274013",
            "text/plain": [
              "[1] 0.6914625"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Gamma Distribution\n",
        "\n",
        "A random variable $Y$ is said to have a *gamma probability distribution*, gamma$(\\alpha, \\beta)$ with shape parameter $\\alpha > 0$ and scale parameter $\\beta > 0$ iff the density function of $Y$ is\n",
        "\n",
        "$$f(y) =\\begin{cases}\n",
        "      \\frac{y^{\\alpha -1} e^-{\\frac{y}{\\beta}}}{\\beta^{\\alpha} \\Gamma(\\alpha) }& 0 \\leq y < \\infty \\\\\n",
        "      0 &  \\text{elsewhere} \\\\  \n",
        "   \\end{cases}$$\n",
        "where,\n",
        "$$ \\Gamma(\\alpha) = \\int_{0}^{\\infty} y^{\\alpha -1}e^{-y}\\,dy$$\n",
        "\n",
        "Furthermore,\n",
        "$$\\mu = E[Y] = \\alpha \\beta$$\n",
        "\n",
        "$$\\sigma^{2} = \\alpha \\beta^{2}$$\n",
        "\n",
        "$$m(t)= \\frac{1}{(1-\\beta t)^{\\alpha}} \\quad \\text{for } t < \\frac{1}{\\beta}$$\n",
        "\n",
        "Defintion\n",
        "* Left skew/left tailed - majority of data on right- tail/flat end on left\n",
        "* Right skew/right tailed - majority of data on left - tail/flat end on right\n",
        "R: `pgamma(y0,α,1/β)`"
      ],
      "metadata": {
        "id": "WuGPp3T3Vwi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pgamma(0.2, 1, 1)\n",
        "#R: pgamma(y0,α,1/β)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "GscPkAvOiaF_",
        "outputId": "3af5d9c8-1ec0-470b-a64d-c4ec6d35aa86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "0.181269246922018"
            ],
            "text/markdown": "0.181269246922018",
            "text/latex": "0.181269246922018",
            "text/plain": [
              "[1] 0.1812692"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exponential Distribution\n",
        "\n",
        "A random variable $Y$ is said to have a *exponential probability distribution*, expon($\\beta$), with parameter $\\beta > 0$ (and $\\alpha =1$) iff the density function of $Y$ is\n",
        "\n",
        "$$f(y) =\\begin{cases}\n",
        "      \\frac{y^{\\alpha -1} e^-{\\frac{y}{\\beta}}}{\\beta^{\\alpha} \\Gamma(\\alpha) } = \\frac{1}{\\beta} e^{-\\frac{y}{\\beta}}& 0 \\leq y < \\infty \\\\\n",
        "      0 &  \\text{elsewhere} \\\\  \n",
        "   \\end{cases}$$\n",
        "where,\n",
        "$$ \\Gamma(1) = \\int_{0}^{\\infty} y^{1-1}e^{-y}\\,dy = 1$$\n",
        "\n",
        "Furthermore,\n",
        "$$\\mu = E[Y] = \\alpha \\beta = 1 \\beta =\\beta$$\n",
        "\n",
        "$$\\sigma^{2} = \\alpha \\beta^{2} = 1 \\beta^{2} = \\beta^{2}$$\n",
        "\n",
        "$$m(t)= \\frac{1}{(1-\\beta t)^{1}} \\quad \\text{for } t < \\frac{1}{\\beta}$$\n",
        "\n",
        "<br>\n",
        "\n",
        "Note: $\\textit{expon}(\\beta) \\sim gamma(\\alpha = 1, \\beta) $"
      ],
      "metadata": {
        "id": "p1SQlly_XyqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Chi-square Distribution\n",
        "\n",
        "Let $v$ be a positive integer. A random variable $Y$ is said to have a *chi-square probability distribution*, $\\chi^{2}$ or chi-square($v$), with $v$ *degrees of freedom* iff $Y$ is a gamma-dstributed random variable with parameters $\\alpha = \\frac{v}{2}$ and $\\beta = 2$. the density function of $Y$ is\n",
        "\n",
        "$$f(y) =\\begin{cases}\n",
        "      \\frac{y^{\\frac{v}{2} -1} e^-{\\frac{y}{ 2}}}{ 2^{\\frac{v}{2}} \\Gamma(\\frac{v}{2}) } & 0 \\leq y < \\infty \\\\\n",
        "      0 &  \\text{elsewhere} \\\\  \n",
        "   \\end{cases}$$\n",
        "where,\n",
        "$$ \\Gamma(\\frac{v}{2}) = \\int_{0}^{\\infty} y^{\\frac{v}{2}-1}e^{-y}\\,dy$$\n",
        "\n",
        "Furthermore,\n",
        "$$\\mu = E[Y] = \\alpha \\beta = \\frac{v}{2} 2 = v$$\n",
        "\n",
        "$$\\sigma^{2} = \\alpha \\beta^{2} = \\frac{v}{2} 2^{2} =  2v$$\n",
        "\n",
        "$$m(t)= \\frac{1}{(1-2t)^{\\frac{v}{2}}} \\quad \\text{for } t < \\frac{1}{\\beta}$$\n",
        "<br>\n",
        "\n",
        "Note: $\\chi^{2}(v) \\sim gamma(\\alpha = \\frac{v}{2}, \\beta = 2) $"
      ],
      "metadata": {
        "id": "wsteiqd5atk9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Beta Distribution\n",
        "\n",
        "A random variable $Y$ is said to have a *beta probability distribution* with parameters $\\alpha > 0$ and scale parameter $\\beta > 0$ iff the density function of $Y$ is\n",
        "\n",
        "$$f(y) =\\begin{cases}\n",
        "      \\frac{y^{\\alpha -1} (1-y)^{-\\beta- 1}}{B (\\alpha, \\beta) }& 0 \\leq y < \\infty \\\\\n",
        "      0 &  \\text{elsewhere} \\\\  \n",
        "   \\end{cases}$$\n",
        "where,\n",
        "$$ B(\\alpha, \\beta) = \\int_{0}^{1} y^{\\alpha -1} (1-y)^{\\beta - 1}\\,dy = \\frac{\\Gamma (\\alpha) \\Gamma (\\beta)}{\\Gamma(a + b)}$$\n",
        "\n",
        "Furthermore,\n",
        "$$\\mu = E[Y] = \\frac{\\alpha}{\\alpha + \\beta}$$\n",
        "\n",
        "$$\\sigma^{2} = \\frac{\\alpha \\beta}{(\\alpha + \\beta) (\\alpha + \\beta + 1)}$$\n",
        "\n"
      ],
      "metadata": {
        "id": "9718bEVfd8Io"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Moment Generating Function\n",
        "If $Y$ is a continuous random variable, then the $k$th *moment about the origin* is\n",
        "$$\\mu_{k}' = E(Y^{k}) \\quad k\\in \\mathbb{N}$$\n",
        "\n",
        "The $k$th *moment about the mean*, or the $k$th *central moment*, is given by\n",
        "$$ \\mu_{k}= E[Y-\\mu]^{k} \\quad k \\in \\mathbb{N}$$\n",
        "\n",
        "<br>\n",
        "\n",
        "If $Y$ is a continuous random varaible, then the *moment-generating function* of $Y$ is given by\n",
        "$$m(t) = E(e^{tY})$$\n",
        "\n",
        "Note: $$E(e^{tY}) = \\int_{-\\infty}^{\\infty}e^{tY} f(y) \\,dy = \\int_{-\\infty}^{\\infty} (1 + tY + \\frac{t^{2}y^{2}}{2!} + \\frac{t^{3} y^{3}}{3!} + ...) f(y) \\,dy = \\int_{-\\infty}^{\\infty} f(y) \\,dy + \\int_{-\\infty}^{\\infty}y f(y) \\,dy + \\frac{t^{2}}{2!}\\int_{-\\infty}^{\\infty}y^{2} f(y) \\,dy+ ... = 1 + t\\mu_{1}'+ \\frac{t^{2}}{2!}\\mu_{2}' + ...$$\n",
        "\n",
        "The moment-generating function exist if $\\exists$ a constant, $b > 0$, s.t. $m(t)$ is finite for $\\lvert t \\rvert  \\leq b$\n",
        "\n",
        "<br>\n",
        "\n",
        "Let $Y$ be a random variable with  ensity function $f(y)$ and $g(Y)$ be a function of $Y$. Then the moment-generating function for $g(Y)$ is\n",
        "$$E[e^{tg(Y)}] = \\int_{-\\infty}^{\\infty} e^{tg(Y)} f(y) \\,dy$$"
      ],
      "metadata": {
        "id": "XxidfQslhj1q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tchebysheff's Theorem\n",
        "\n",
        "Tchebysheff's Theorem Let $Y$ be a random variable with finite mean $\\mu$ and variance $\\sigma^{2}$. Then, for any $k > 0$,\n",
        "\n",
        "$$P (\\lvert Y - \\mu \\rvert < k\\sigma) \\geq 1 - \\frac{1}{k^{2}}$$\n",
        "<center> or</center>\n",
        "\n",
        "$$P(\\lvert Y - \\mu \\rvert \\geq k\\sigma) \\leq \\frac{1}{k^{2}}$$"
      ],
      "metadata": {
        "id": "y9bJTWGAATZL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter 5- Multivariate\n",
        "* Bivariate and Multivariate Distributions\n",
        "* Marginal and Conditional Distributions\n",
        "* Independent Random Variables\n",
        "* Expected Value\n",
        " * Special Theorems\n",
        "* Covariance of Two Random Variables\n",
        "* Expected Value and Variance of Linear Functions\n",
        "\n",
        "\n",
        "* The Multinomial Distribution\n",
        "* The Bivariate Normal Distribution\n",
        "* Conditional Expectations"
      ],
      "metadata": {
        "id": "E_hAhEtYc3XV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bivariate and Multivariate Distributions\n",
        " ---\n",
        "Let $Y_{1}$ and $Y_{2}$ be discrete random variables. The *joint(bivariate) probability function* for  $Y_{1}$ and $Y_{2}$ is given by\n",
        "\n",
        "$$p(y_{1}, y_{2}) = P(Y_{1}= y_{1}, Y_{2} =y_{2}), \\quad -\\infty < y_{1}, y_{2} <\\infty$$\n",
        "<br>\n",
        "For any random variables $Y_{1}$ and $Y_{2}$, the joint distribution function $F(y_{1}, y_{2})$ is\n",
        "$$F(y_{1}, y_{2})= P(Y_{1} \\leq y_{1}, Y_{2} \\leq y_{2}), \\quad -\\infty < y_{1}, y_{2} < \\infty$$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AIq8acvseU_5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Discrete\n",
        "---\n",
        "If $Y_{1}$ and $Y_{2}$ are discrete random variables with joint probability function $p_{y_{1}, y_{2}}(y_{1}, y_{2})$, then\n",
        "\n",
        "1. $p(y_{1}, y_{2}) \\geq  0  \\quad \\forall y_{1}, y_{2}$\n",
        "2. $\\sum\\limits_{y_{1}, y_{2} }p(y_{1}, y_{2}) = 1$\n",
        " * The sum of all values $(y_{1}, y_{2})$ that are assigned nonzero probabilities = 1\n",
        "3. $F_{Y_{1} Y_{2}} (y_{1}, y_{2}) = \\sum\\limits_{u_{1} \\leq y_{1}, u_{2} \\leq y_{2}} p_{Y_{1}, Y_{2}} (u_{1}, u_{2})= \\sum\\limits_{u_{1} \\leq y_{1}} \\sum\\limits_{ u_{2} \\leq y_{2}} p_{Y_{1}, Y_{2}} (u_{1}, u_{2})$"
      ],
      "metadata": {
        "id": "f2MXixO_iErq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Continuous\n",
        "---\n",
        "Let $Y_{1}$ and $Y_{2}$ be continuous random variables with joint distribution function $F(y_{1}, y_{2})$. If there exists a nonnegative function $f(y_{1}, y_{2})$, s.t.\n",
        "$$F(y_{1}, y_{2}) = P(Y_{1} \\leq y_{1} , Y_{2} \\leq y_{2} )=\\int\\limits_{-\\infty}^{y_{1} } \\int\\limits_{-\\infty}^{y_{2} }f(t_{1}, t_{2}) \\,dt_{2} \\,dt_{1} \\quad -\\infty < y_{1}, y_{2} <\\infty$$\n",
        "then $Y_{1}$ and $Y_{2}$ are said to be *jointly continuious random variables*\n",
        "\n",
        "Note: $f(y_{1},y_{2}) = P \\{ Y_{1} =y_{1}, Y_{2} = y_{2} \\}$ is called the *joint probability density function*.\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "If $Y_{1}$ and $Y_{2}$ are jointly continuous random vairables with a joint denisty function given by $f(y_{1}, y_{2})$, then\n",
        "1. $f(y_{1}, y_{2}) \\geq 0 \\quad \\forall y_{1}, y_{2}$\n",
        "2. $\\int\\limits_{-\\infty}^{\\infty} \\int\\limits_{-\\infty}^{\\infty}f(y_{1}, y_{2}) \\,dy_{1} \\,dy_{2} = 1$\n",
        "\n",
        "####Properties of $F(y_{1}, y_{2})$\n",
        "\n",
        "1. $F(-\\infty, -\\infty)=F( y_{1}, -\\infty)= F(-\\infty, y_{2}) = 0$\n",
        "2. $F(\\infty, \\infty) = 1$\n",
        "3. If $y_{1}^{*} \\geq y_{1}$ and $y_{2}^{*} \\geq y_{2}$, then $F(y_{1}^{*}, y_{2}^{*}) - F(y_{1}^{*}, y_{2}) - F(y_{1}, y_{2}^{*}) + F(y_{1}, y_{2}) \\geq 0 $"
      ],
      "metadata": {
        "id": "k3i4MkSmiF6a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Marginal and Conditional Distributions\n",
        "\n",
        "###Discrete\n",
        "---\n",
        "Let $Y_{1}$ and $Y_{2}$ be jointly discrete random variables with probability function $p(y_{1}, y_{2})$. Then the *marginal probability functions* of $Y_{1}$ and $Y_{2}$, respectively are given by\n",
        "$$ p_{y_{1} }(y_{1}) = p_{1}(y_{1}) = \\sum\\limits_{y_{2}}p(y_{1}, y_{2}) \\quad p_{y_{2}}(y_{2}) = p_{2}(y_{2}) = \\sum\\limits_{y_{1} }p(y_{1}, y_{2}) $$\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "The *conditional discrete probability function* of $Y_{1}$ given $Y_{2}$ is\n",
        "$$p_{y_{1} \\vert y_{2}}(y_{1} \\vert y_{2}) = P(Y_{1} = y_{1} \\vert Y_{2} = y_{2}) = \\frac{P(Y_{1}=y_{2}, Y_{2}=y_{2})}{P(Y_{2} = y_{2})} = \\frac{p_{y_{1}, y_{2}}(y_{1}, y_{2})}{p_{y_{2}}(y_{2})} \\quad \\text{ if } p_{y_{2}} > 0$$\n",
        "\n",
        "Note: $p_{y_{1} \\vert y_{2}}$ is *undefined* if $p_{2}(y_{2}) = 0$\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "###Continuous\n",
        "---\n",
        "Let $Y_{1}$ and $Y_{2}$ be jointly continuous random variables with joint probability density function $f(y_{1}, y_{2})$. Then the *marginal probability functions* of $Y_{1}$ and $Y_{2}$, respectively are given by\n",
        "$$f_{1}(y_{1}) = \\sum\\limits_{y_{2} }f(y_{1}, y_{2}) \\quad f_{2}(y_{2}) = \\sum\\limits_{y_{1} }f(y_{1}, y_{2})$$\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "If $Y_{2}$ and $Y_{2}$ are jointly continuous random variables with joint density function $f(y_{1}, y_{2})$, then the *conditional distribution function* of $Y_{1}$ given $Y_{2} = y_{2}$ is\n",
        "$$F(y_{1} \\vert y_{2}) = P(Y_{1} \\leq y_{1} \\vert Y_{2} = y_{2})$$\n",
        "\n",
        "<br>\n",
        "\n",
        "The *conditional density function* of $Y_{1}$ given $Y_{2} = y_{2}$ is\n",
        "$$f_{y_{1} \\vert y_{2}}(y_{1} \\vert y_{2}) = \\frac{f(y_{1}, y_{2})}{f_{y_{2}}(y_{2})} \\quad \\text{ if } f_{y_{2}} > 0$$\n",
        "Note: $f_{y_{1} \\vert y_{2}}$ is *undefined* if $f_{2}(y_{2}) = 0$\n"
      ],
      "metadata": {
        "id": "WMs8Yg-SKasd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Independent Random Variables\n",
        "\n",
        "Let $Y_{1} $ have distribution function $F_{y_{1}}(y_{1})$ and $Y_{2} $ have distribution function $F_{y_{2} }(y_{2})$ and $Y_{1}, Y_{2}$ have a joint distribution function $F(y_{1}, y_{2})$. Then Y_{1} and Y_{2} are *independent* iff\n",
        "$$F(y_{1}, y_{2}) = F_{y_{1}}(y_{1}) \\cdot F_{y_{2}}(y_{2}) \\quad \\forall (y_{1}, y_{2})$$\n",
        "\n",
        "If $Y_{1}$ and $Y_{2}$ are not independent, they are said to be *dependent*.\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "###Discrete\n",
        "If $Y_{1}$ and $Y_{2}$ are discrete random variables with joint probability function $p(y_{1}, y_{2})$ and marginal probability functions $p_{y_{1}}(y_{1})$ and $p_{y_{2}}(y_{2})$, respectively,\n",
        "then $Y_{1}$ and $Y_{2}$ are independent iff\n",
        "$$p_{y_{1}, y_{2}}(y_{1}, y_{2}) = p_{y_{1}}(y_{1}) \\cdot p_{y_{2}}(y_{2}) \\quad \\forall (y_{1}, y_{2})$$\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "###Continuous\n",
        "If $Y_{1}$ and $Y_{2}$ are continuous random variables with joint probability density function $f(y_{1}, y_{2})$ and marginal probability density functions $f_{y_{1}}(y_{1})$ and $f_{y_{2}}(y_{2})$, respectively,\n",
        "then $Y_{1}$ and $Y_{2}$ are independent iff\n",
        "$$f_{y_{1}, y_{2}}(y_{1}, y_{2}) = f_{y_{1}}(y_{1}) \\cdot f_{y_{2}}(y_{2}) \\quad \\forall (y_{1}, y_{2})$$"
      ],
      "metadata": {
        "id": "dZTzT9-9WIjA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $Y_{1}$ and $Y_{2}$ have a joint pdf, $f(y_{1}, y_{2})$ that is positive iff $a \\leq y_{1} \\leq b$ and $c \\leq y_{2} \\leq d$, for constants $a, b, c,$ and $d$; and $f(y_{1}, y_{2}) = 0$. Then Y_{1} and Y_{2} are independent random variables iff\n",
        "$$f(y_{1}, y_{2}) = g(y_{1})h(y_{2})$$\n",
        "where $g(y_{1})$ and $h(y_{2})$ are nonnegativefunctions."
      ],
      "metadata": {
        "id": "MDQSTsWaKFCb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Expected Value\n",
        "\n",
        "###Discrete\n",
        "---\n",
        "Let $g(Y_{1}, Y_{2},..., Y_{k})$ be a function of the discrete random variables, $Y_{1}, Y_{2},..., Y_{k}$, which have probability function $p(y_{1}, y_{2},..., y_{k})$. Then the expected value of $g(Y_{1}, Y_{2},..., Y_{k})$ is\n",
        "$$E[g(Y_{1}, Y_{2},..., Y_{k})] = \\sum\\limits_{y_{k}}... \\sum\\limits_{y_{2}}\\sum\\limits_{y_{1}} g(y_{1}, y_{2},..., y_{k}) p(y_{1}, y_{2},..., y_{k})$$\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "###Continuous\n",
        "---\n",
        "\n",
        "Let $g(Y_{1}, Y_{2},..., Y_{k})$ be a function of the continuous random variables, $Y_{1}, Y_{2},..., Y_{k}$, which have pdf, $f(y_{1}, y_{2},..., y_{k})$. Then the expected value of $g(Y_{1}, Y_{2},..., Y_{k})$ is\n",
        "$$E[g(Y_{1}, Y_{2},..., Y_{k})] = \\int\\limits_{-\\infty}^{\\infty}... \\int\\limits_{-\\infty}^{\\infty}\\int\\limits_{-\\infty}^{\\infty} g(y_{1}, y_{2},..., y_{k}) f(y_{1}, y_{2},..., y_{k}) \\,dy_{1}\\,dy_{2} ...\\,dy_{k}$$"
      ],
      "metadata": {
        "id": "UYKlAOwhLWIh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Special Theorems\n",
        "---"
      ],
      "metadata": {
        "id": "R91RMZGuyBw7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $c$ be a constant. Then\n",
        "$$E(c) = c$$\n"
      ],
      "metadata": {
        "id": "qCJ1hKLmOhgv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $g(Y_{1}, Y_{2})$ be a function of the random variables $Y_{1}$ and $Y_{2}$ and let $c$ be a constant. Then\n",
        "$$E[cg(Y_{1}, Y_{2})] = cE[g(Y_{1}, Y_{2})]$$"
      ],
      "metadata": {
        "id": "KfE2u-B7PDe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $Y_{1}$ and $Y_{2}$ be random variables and $g_{1}(Y_{1}, Y_{2}), g_{2}(Y_{1}, Y_{2}), ... , g_{k}(Y_{1}, Y_{2})$ be functions of $Y_{1}$ and $Y_{2}$. Then\n",
        "$$E[g_{1}(Y_{1}, Y_{2}) + g_{2}(Y_{1}, Y2) +···+ g_{k} (Y_{1}, Y_{2})]\n",
        "= E[g_{1}(Y_{1}, Y_{2})] + E[g_{2}(Y_{1}, Y_{2})] +···+ E[g_{k} (Y_{1}, Y_{2})].$$"
      ],
      "metadata": {
        "id": "p1sTiV0HPEeh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $Y_{1}$ and $Y_{2}$ be independent random variables and $g(Y_{1})$ and $h(Y_{2})$ be functions of only $Y_{1}$ and $Y_{2}$, respectively. Then\n",
        "$$E[g(Y_{1})h(Y_{2})] = E[g(Y_{1})]E[h(Y_{2})]$$\n",
        "$$E[Y_{1} Y_{2}] = E[Y_{1}] E[Y_{2}]$$"
      ],
      "metadata": {
        "id": "AawUPcW6P9lS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IUclW_V1_Kp2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Covariance\n",
        "---\n",
        "If $Y_{2}$ and $Y_{2}$ are random variables with means $\\mu_{1}$ and $\\mu_{2}$, respectively, the *covariance* of $Y_{1}$  and $Y_{2}$ is\n",
        "$$Cov(Y_{1}, Y_{2}) = E[(Y_{1} - \\mu_{1})(Y_{2}-\\mu_{2})]=E(Y_{1}Y_{2})- E(Y_{1})E(Y_{2})$$\n",
        "\n",
        "Note: Covariance provides a measure of the linear dependence between $Y_{1}$ and $Y_{2}$\n",
        "\n",
        "$$\\rho = \\frac{Cov(Y_{1}, Y_{2})}{ \\sigma_{1}\\sigma_{2}}$$\n",
        "\n",
        "\n",
        "Definition\n",
        "* Positive Correlation: $\\rho > 0$\n",
        "* Negative Correlation: $\\rho < 0$\n",
        "* Zero Correlation/ Uncorrelated: $\\rho = 0$  "
      ],
      "metadata": {
        "id": "th8NkssDLitD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If $Y_{1}$ and $Y_{2}$ are independent random variables, then\n",
        "$$Cov(Y_{1}, Y_{2}) = 0.$$"
      ],
      "metadata": {
        "id": "3OWq4t4Qz26K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Expected Value and Variance of Linear Functions\n",
        "\n",
        "Let $Y_{1}, Y_{2},..., Y_{n}$ and $X_{1}, X_{2},..., X_{m}$ be random variables with $E(Y_{i}) = \\mu_{i}$\n",
        "and $E(X_{j}) = \\xi_{j}$. Let\n",
        "$U_{1} =\\sum\\limits_{i=1}^{n} a_{i}Y_{i}$\n",
        "and $U_{2} =\\sum\\limits_{j=1}^{m} a_{j}X_{j}$\n",
        "where $a_{1}, a_{2},..., a_{n}$ and $b_{1}, b_{2},..., b_{m}$ are constants. Then the following hold\n",
        "1. $E(U_{1}) = \\sum\\limits_{i=1}^{n} a_{i}\\mu_{i}$\n",
        "2. $V(U_{1}) = \\sum\\limits_{i=1}^{n} a_{i}^{2} V(Y_{i}) + 2 \\sum \\sum_{1 \\leq i < j \\leq n} a_{i} a_{j} Cov(Y_{i}, Y_{j})$\n",
        "3. $Cov(U_{1}, U_{2}) = \\sum\\limits_{i=1}^{n} \\sum\\limits_{j=1}^{m} a_{i}b_{j} Cov(Y_{i}, X_{j})$\n",
        "\n",
        "Note: for 2, it's the double sum is over all pairs $(i, j)$ with $i < j$.\n",
        "\n",
        "<br>\n",
        "\n",
        "Let $Y_{1}, Y_{2}, . . . , Y_{n}$ be\n",
        "uncorrelated random variables. Then\n",
        "$$V\\bigg[\\sum\\limits_{j=1}^{n} Y_{j}\\bigg] = \\sum\\limits_{j=1}^{n} V[Y_{j}]$$\n"
      ],
      "metadata": {
        "id": "IYaPmHmt0OEl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Multinomial\n",
        "\n",
        "Assume that $p_{1}, p_{2},..., p_{k}$ are such that $\\sum_{i=1}^{k}p_{i} = 1$, and $p_{i} > 0$ for $i = 1, 2,..., k$. The random variables $Y_{1}, Y_{2},..., Y_{k}$, are said to have a multinomial distribution with parameters $n$ and $p_{1}, p_{2},..., p_{k}$ if the joint probability function of Y_{1}, Y_{2},..., Y_{k} is given by\n",
        "$$p(y_{1}, y_{2},..., y_{k}) = \\frac{n!}{\n",
        "y_{1}!y_{2}! ··· y_{k}!}\n",
        "p_{1}^{y_{1}}p_{2}^{y_{2}} ··· p_{k}^{y_{k}}, \\quad \\text{where, for each } i, y_{i} = 0, 1, 2,..., n \\text{ and } \\sum_{i=1}^{k} y_{i} = n.\n",
        "$$\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "A *multinomial experiment* possesses the following properties:\n",
        "1. The experiment consists of $n$ identical trials.\n",
        "2. The outcome of each trial falls into one of $k$ classes or cells.\n",
        "3. The probability that the outcome of a single trial falls into cell $i$, is $p_{i}, i = 1, 2,..., k$ and remains the same from trial to trial.\n",
        " * Note that $p_{1} + p_{2} + p_{3} +···+ p_{k} = 1$.\n",
        "4. The trials are independent.\n",
        "5. The random variables of interest are $Y_{1}, Y_{2},..., Y_{k},$ where $Y_{i}$ equals the number of trials for which the outcome falls into cell $i$.\n",
        "* Note that $Y_{1} + Y_{2} + Y_{3} +···+ Y_{k} = n$.\n",
        "6. $\\sum\\limits_{i=1}^{k}p_{i} = 1$\n",
        "\n"
      ],
      "metadata": {
        "id": "0BRaRseU3ric"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If $Y_{1}, Y_{2},..., Y_{k}$ have a multinomial distribution with parameters $n$ and $p_{1}, p_{2},..., p_{k}$, then\n",
        "1. $E(Y_{i}) = np_{i}$\n",
        "2. $V(Y_{i}) = np_{i}q_{i}$\n",
        "2. $Cov(Y_{s}, Y_{t}) = -np_{s}p_{t} \\quad$, if $s \\neq t$."
      ],
      "metadata": {
        "id": "79CYgOza618p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Bivariate Normal Distribution\n",
        " ..."
      ],
      "metadata": {
        "id": "NFwDp5Ux78Kg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Conditional Expectations\n",
        "If $Y_{1}$ and $Y_{2}$ are any two joint random variables, the conditional expectation of $g(Y_{1})$, given that $Y_{2} = y_{2}$, is defined to be\n",
        "###Discrete\n",
        "\n",
        "$$E(g(Y_{1}) \\vert Y_{2} =y_{2}) = \\sum_{y_{1}} g(y_{1})p(y_{1} \\vert y_{2})$$\n",
        "\n",
        "####Continuous\n",
        "$$E(g(Y_{1})| Y_{2} = y2) =\n",
        "\\int_{-\\infty}^{\\infty} g(y_{1}) f(y_{1} \\vert y_{2}) \\,dy_{1}$$\n"
      ],
      "metadata": {
        "id": "EMJo3CDb8CsF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $Y_{1}$ and $Y_{2}$ denote random variables. Then\n",
        "$$E(Y_{1}) = E[E(Y_{1} \\vert Y_{2})]$$\n",
        "$$V(Y_{1}) = E[V(Y_{1} | Y_{2})] + V[E(Y_{1} | Y_{2})]$$\n",
        "$$V[Y_{1} \\vert Y_{2}] = E[ (Y_{1} - E[Y_{1}\\vert Y_{2}])^{2} \\vert Y_{2}]$$"
      ],
      "metadata": {
        "id": "LO7PYfCq9mt1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Miscellaneous\n",
        "\n",
        "####Conditional Expectations and Conditional Variance\n",
        "\n",
        " If $(Ω, P)$ is a probability space and $B ⊆ Ω$ is an event that satisfies $P(B) > 0$, then the function $Q(·)$,\n",
        "defined as $Q(A) = P(A | B)$ for $A ⊆ Ω$, is a probability measure on $Ω$\n",
        "\n",
        "#####Properties of Q-\n",
        "* Q-CDF (w/ random variable $Y$) is $F_{Y}^{Q}(y) = Q\\{ Y \\leq y \\} = P \\{ Y \\leq y \\vert B \\}$\n",
        "* Q-PMF of discrete random item $X$ is $p_{X}^{Q} = Q \\{ X =x \\} = P\\{X = x \\vert B\\}$\n",
        "* Joint Q-PDF of random item $X_{1}, X_{2}$ is $p^{Q}_{X_{1},X_{2}} (x_{1}, x_{2}) = Q \\{ X_{1} = x_{1}, X_{2} = x_{2} \\}  = P\\{X_{1} = x_{1}, X_{2} = x_{2} | B \\} $\n",
        "* Q-Expectation is $E^{Q}[Y] = \\int\\limits_{-\\infty}^{\\infty} y f_{Y}^{Q} \\,dy$\n",
        "* Q-Variance is $V^{Q}[Y] = E^{Q} [(Y=E^{Q}[Y])^{2}]$\n",
        "* Q-MGF is $m^{Q}(t) = E^{Q} [ e^{tY}]$\n"
      ],
      "metadata": {
        "id": "cSMydp1o3ewk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter 6- Functions of Random Variables\n",
        "* Distribution of a Function\n",
        "of Random Variables\n",
        "* Method of Distribution Functions\n",
        "* Method of Transformations\n",
        "* Method of Moment-Generating Functions\n",
        "* Multivariable Transformations Using Jacobians\n",
        "* Order Statistics"
      ],
      "metadata": {
        "id": "7bJ2gJvRdc1Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Distribution of a Function of Random Variables\n",
        "Consider random variables $Y_{1}, Y_{2},..., Y_{n}$ and a function $U(Y_{1}, Y_{2},..., Y_{n})$, denoted simply as $U$. Then three of the methods for finding the probability distribution\n",
        "of $U$ are as follows:\n",
        "1. Method of distribution functions\n",
        "2. Method of transformations\n",
        "3. Method of moment-generating functions"
      ],
      "metadata": {
        "id": "mpZEptFf-kGE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Method of distribution functions\n",
        "---\n",
        "1. Find the region in the $y_{1}, y_{2},..., y_{n}$ space for which $U \\leq u$ and then find $P(U \\leq u)$\n",
        "2. Find the distribution function for $U,F_{U}(u) = P(U \\leq u)$ by integrating $f(y_{1}, y_{2},..., y_{n})$ over this region.\n",
        "3. Differentiate the distribution\n",
        "function, $F_{U}(u)$ to obtain the density function for U ."
      ],
      "metadata": {
        "id": "ZQCCOppE_K5j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example- Univariate Case\n",
        "Suppose that $Y$ has density function given by\n",
        "$$ f(y) \\begin{cases}\n",
        "      2y &  0\\leq y \\leq 1 \\\\\n",
        "      0 &  \\text{else} \\\\  \n",
        "   \\end{cases}\n",
        "$$\n",
        "Let $U = 3Y - 1$. Find the probability density function for $U$.\n",
        "\n",
        "####Step 1: Find the region of $F_{U}(u)$\n",
        "---\n",
        "$$F_{U}(u) = P(U \\leq u) = P(3Y - 1 \\leq u) = P(Y \\leq \\frac{u + 1}{3})$$\n",
        "<br>\n",
        "\n",
        "$0\\leq \\frac{u + 1}{3}$$ \\leq 1 ⇒ -1\\leq u \\leq 2 $\n",
        "#### Step 2: Find $F_{U}(u)$\n",
        "---\n",
        "$$P(Y \\leq \\frac{u + 1}{3})= \\int_{-\\infty}^{\\frac{u+1}{3}} f(y) \\,dy = \\int_{0}^{\\frac{u+1}{3}} 2y \\,dy = (\\frac{u+1}{3})^{2}$$\n",
        "\n",
        "$$ F_{U}(u) \\begin{cases}\n",
        "      0 &  u < -1 \\\\\n",
        "      (\\frac{u+1}{3})^{2} &  -1\\leq u \\leq 2 \\\\\n",
        "      1 &  u > 2 \\\\  \n",
        "   \\end{cases}\n",
        "$$\n",
        "####Step 3: Find pdf\n",
        "---\n",
        "$$ f_{U}(u) \\frac{F_{U}(u)}{\\,du} \\begin{cases}\n",
        "      \\frac{2(u+1)}{9} &  -1\\leq u \\leq 2 \\\\\n",
        "      0 &  \\text{elsewhere} \\\\  \n",
        "   \\end{cases}\n",
        "$$"
      ],
      "metadata": {
        "id": "kzeru2q3AAge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example- Bivariate Case\n",
        "Suppose that $Y$ has density function given by\n",
        "$$ f(y_{1}, y_{2}) \\begin{cases}\n",
        "      3y_{1} &  0\\leq y_{2} \\leq y_{1} \\leq 1 \\\\\n",
        "      0 &  \\text{else} \\\\  \n",
        "   \\end{cases}\n",
        "$$\n",
        "Let $U = Y_{1} - Y_{2}$. Find the probability density function for $U$.\n",
        "\n",
        "####Step 1: Find the region of $F_{U}(u)$\n",
        "---\n",
        "Region: $y_{1} - y_{2} \\leq u$\n",
        "$$F_{U}(u) = P(U \\leq u) = P(Y_{1} - Y_{2} \\leq u)$$\n",
        "<br>\n",
        "\n",
        "#### Step 2: Find $F_{U}(u)$\n",
        "---\n",
        "$$F_{U}(u) = P(U \\leq u)= 1- P(U \\geq u)= 1- \\int_{u}^{1} \\int_{0}^{y_{1} -u} 3y_{1} \\,dy_{2}\\,dy_{1} = \\frac{(3u-u^{3})}{2}$$\n",
        "\n",
        "$$ F_{U}(u) \\begin{cases}\n",
        "      0 &  u < 0 \\\\\n",
        "      \\frac{(3u-u^{3})}{2} &  0\\leq u \\leq 1 \\\\\n",
        "      1 &  u > 1 \\\\  \n",
        "   \\end{cases}\n",
        "$$\n",
        "####Step 3: Find pdf\n",
        "---\n",
        "$$ f_{U}(u) \\frac{F_{U}(u)}{\\,du} \\begin{cases}\n",
        "      \\frac{3(1-u^{2})}{2} &  0\\leq u \\leq 1 \\\\\n",
        "      0 &  \\text{elsewhere} \\\\  \n",
        "   \\end{cases}\n",
        "$$\n",
        "\n",
        "Note:$$E[U] = \\int_{0}^{1}u(\\frac{3(1-u^{2})}{2})\\,du = \\frac{3}{8}$$"
      ],
      "metadata": {
        "id": "n_kFLaN8Y8DH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Method of Transformations\n",
        "---\n",
        " Let $U = h(Y)$, where $h(y)$ is either an increasing or decreasing function of $y$ for all $y$ such that $f_{Y}(y) > 0$.\n",
        " 1. Find the inverse function, $y= h^{-1}(u)$\n",
        " 2. Evaluate $\\frac{\\,d h^{-1}(u)}{\\,du} =$\n",
        " 3. Find $f_{U}(u)$ by $$f_{U}(u) = f_{Y}[h^{-1}(u)] \\bigg\\lvert \\frac{\\,d h^{-1}}{\\,du} \\bigg\\rvert$$"
      ],
      "metadata": {
        "id": "uX3UyqL9nOXz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Method of Moment-Generating Functions\n",
        "Let $U$ be a function of the random variables $Y_{i}, Y_{2}, Y_{n}$\n",
        "1. Find mgf for $U, m_{U}(t)$\n",
        "2. Compare with other mgf to determine distribution\n",
        "\n",
        "\n",
        "Let $m_{X}(t)$ and $m_{Y}(t)$ denote the moment-generating functions of random variables $X$ and $Y$ , respectively. If both moment-generating functions exist and $m_{X}(t) = m_{Y}(t)$ for all values of $t$, then $X$ and $Y$ have the same probability distribution.\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "Let $Y_{1}, Y_{2},..., Y_{n}$ be independent random variables with moment-\n",
        "generating functions $m_{Y_{1}}(t), m_{Y_{2}}(t), . . . , m_{Y_{n}}(t)$, respectively. If $U = Y_{1} + Y_{2} +···+ Y_{n}$, then\n",
        "$$m_{U}(t) = m_{Y_{1}}(t) \\times m_{Y_{2}} (t) \\times ··· \\times m_{Y_{n}}(t).$$\n"
      ],
      "metadata": {
        "id": "8adp--IRp9_V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $Y_{1}, Y_{2},..., Y_{n}$ be independent normally distributed random variables with $E(Y_{i}) = \\mu_{i}$ and $V(Y_{i}) = \\sigma_{i}^{2}$ for $i = 1, 2,..., n$, and let $a_{1}, a_{2},..., a_{n}$ be constants. If\n",
        "$$U = \\sum_{i=1}^{n}a_{i}Y_{i} $$\n",
        "\n",
        "then $U$ is normally distributed random variable with\n",
        "$$E(U) = \\sum_{i=1}^{n}a_{i}\\mu_{i}$$\n",
        "$$V(U) = \\sum_{i=1}^{n}a_{i}^{2}\\sigma_{i}^{2}$$\n",
        "\n",
        "\n",
        "Consequently,\n",
        "\n",
        "$$Z_{i} = \\frac{Y_{i} - \\mu_{i}}{\\sigma_{i}}, \\quad i = 1, 2, ..., n$$\n",
        "\n",
        "Then $\\sum\\limits_{i=1}^{n}Z_{i}^{2}$ had a $\\chi^{2}$ distribution with $n$ degrees of freedom."
      ],
      "metadata": {
        "id": "uIoWTvr4rPXp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Multivariable Transformation Using Jacobians"
      ],
      "metadata": {
        "id": "PpXsW0FutK74"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Bivariate Transformation Method\n",
        "\n",
        "Let $Y_{1}$ and $Y_{2}$ are continuous random variables with joint density\n",
        "function $f_{Y_{1},Y_{2}} (y_{1}, y_{2})$ and $ \\forall (y_{1}, y_{2}),  f_{Y_{1},Y_{2}} (y_{1}, y_{2}) > 0$,\n",
        "$$u_{1} = h_{1}(y_{1}, y_{2}) =h_{y_{1}, y_{2}} (y_{1}, y_{2}) \\text{ and } u_{2} = h_{2}(y_{1}, y_{2})$$\n",
        "is a one-to-one transformation from $(y_{1}, y_{2})$ to $(u_{1}, u_{2})$ with inverse\n",
        "$$y_{1} = h^{-1}_{1} (u_{1}, u_{2})\\text{ and } y_{2} = h^{-1}_{2}(u_{1}, u_{2})$$\n",
        "\n",
        "If $h^{-1}_{1} (u_{1}, u_{2})$ and $h^{-1}_{2}(u_{1}, u_{2})$ have continuous partial derivatives wit respect to $u_{1}$ and $u_{2}$ and *Jacobian*\n",
        "\n",
        "$$J = \\begin{bmatrix}\n",
        "\\frac{\\partial h^{-1}_{1}}{\\partial u_{1}} & \\frac{\\partial h^{-1}_{}}{\\partial u_{2}} \\\\\n",
        "\\frac{\\partial h^{-1}_{2}}{\\partial u_{1}} & \\frac{\\partial h^{-1}_{2}}{\\partial u_{2}}\n",
        "\\end{bmatrix}\t= \\frac{\\partial h^{-1}_{1}}{\\partial u_{1}} \\frac{\\partial h^{-1}_{2}}{\\partial u_{2}} - \\frac{\\partial h^{-1}_{2}}{\\partial u_{1}}\\frac{\\partial h^{-1}_{1}}{\\partial u_{2}} \\neq 0$$\n",
        "\n",
        "Then the joint density of $U_{1}$ and $U_{2}$ is\n",
        "$$f_{U_{1}, U_{2}}(u_{1}, u_{2}) = f_{Y_{1}, Y_{2}}(h_{1}^{-1}(u_{1}, u_{2}), h_{2}^{-1}(u_{1}, u_{2})) \\lvert J \\rvert $$\n"
      ],
      "metadata": {
        "id": "ZohtO7NUvqU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Order Statisics"
      ],
      "metadata": {
        "id": "WMz1IJ1z85dH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $Y_{1}, Y_{2},..., Y_{n}$ denote independent continuous random variables with distribution function $F(y)$ and density function $f(y)$. We denote the ordered random variables $Y_{i}$ by $Y(1), Y(2),..., Y(n)$, where $Y(1) \\leq Y(2) \\leq ···\\leq Y(n)$. (Equality signs can be ignored.) Thus,\n",
        "\n",
        "$$Y_{(1)} = \\min(Y_{1}, Y_{2},..., Y_{n}) $$\n",
        "is the minimum of the random variables $Y_{i}$, and\n",
        "$$Y_{(n)} = \\max(Y_{1}, Y_{2},..., Y_{n})$$\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "1. $F_{Y_{(n)}}(y) = P(Y_{(n)} \\leq y) = P(Y_{1} \\leq y, Y_{2} \\leq y, ..., Y_{n} \\leq y)=P(Y_{1} \\leq y)P(Y_{2} \\leq y) ··· P(Y_{n} \\leq y) = [F(y)]^{n}$\n",
        "\n",
        "2. $g_{(n)}(y) = \\frac{\\,d F_{Y_{(n)}}(y)}{\\,dy}= n[F(y)]^{n-1} f(y)$\n",
        "\n",
        "3. $F_{Y_{(1)}}(y) = P(Y_{(1)} \\leq y) = 1 - P(Y_{(1)} > y) = 1 - P(Y_{1} > y. Y_{2} > y, ..., Y_{n} > y) = 1 - [P(Y_{1}) P(Y_{2} > y)... P(Y_{n} > y)] = 1- [1- F(y)]^{n}$\n",
        "4. $g_{(1)}(y) = n[1 − F(y)]^{n-1} f(y)$.\n",
        "5. $g_{(1)(2)...(n)}(y_{1}, y_{2}, ..., y_{n}) = \\begin{cases}\n",
        "      n! f(y_{1}) f(y_{2})...f(y_{n}) &  y_{1} \\leq y_{2} \\leq ... \\leq y_{n} \\\\\n",
        "      0 & elsewhere \\\\  \n",
        "   \\end{cases}\n",
        "   $\n",
        "\n"
      ],
      "metadata": {
        "id": "Gkg8ONqk9hT5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $Y_{1},..., Y_{n}$ be independent identically distributed continuous random variables with common distribution function $F(y)$ and common density function, $f(y)$. If $Y_{(k)}$ denotes the $k$th-order statistic, then the density function of $Y_{(k)}$ is\n",
        "given by\n",
        "\n",
        "$$g_{(k)}(y_{k}) = \\frac{n!}{(k-1)!(n-k)!}[F(y_{k})]^{k-1}[1-F(y_{k})]^{n-k}f(y_{k})$$\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "If $j$ and $k$ are two integers such that $1 \\leq j < k \\leq n$, the joint density of $Y_{(j)}$ and $Y_{(k)}$ is given by\n",
        "\n",
        "$$g_{(j)(k)}(y_{j}, y_{k}) = \\frac{n!}{(j-1)!(k-1-j)!(n-k)!}[F(y_{j})]^{j-1}\\times F(y_{k})1-F(y_{j})]^{k-1-j}\\times [1-F(y_{k})]^{n-k}f(y_{j})f(y_{k}), \\quad -\\infty < y_{j} < y_{k} < \\infty$$"
      ],
      "metadata": {
        "id": "hN3m6U7Pfybu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter 7\n",
        "* Limits for Sequences of Random Variables\n",
        "* Laws of Large Numbers\n",
        "* Sampling Distributions Related to the Normal Distribution\n",
        "* The Central Limit Theorem\n",
        "* The Normal Approximation to the Binomial Distribution"
      ],
      "metadata": {
        "id": "RWQgsUxjDUYz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Limits for Sequences of Random Variables/Modes of Convergence\n",
        "---\n",
        "Let $Y_{n} (n \\in \\mathbb{N})$ and $Y$ be random variables on a probability space $(\\Omega , P)$ Then ...\n",
        "\n",
        "1. $Y$ is the *pointwise limit* of $Y_{n}$ or $Y_{n}$ *converges pointwise* to $Y$ (denoted as $Y_{n} \\xrightarrow[]{\\text{pw}} Y$ or $pw - \\lim\\limits_{n \\rightarrow ∞}Y_{n} = Y$), if $$\\lim_{n \\rightarrow \\infty} Y_{n}(\\omega) = Y(\\omega), \\quad \\forall \\omega \\in \\Omega$$\n",
        "2. $Y$ is the *almost sure  limit* of $Y_{n}$ or $Y_{n}$ *converges almost surely* to $Y$( denoted as $Y_{n} \\xrightarrow[]{\\text{a.s.}} Y$ or $pw - \\lim\\limits_{n \\rightarrow ∞}Y_{n} = Y$), if $$ P \\{ \\omega \\in \\Omega : \\lim_{n \\rightarrow \\infty} Y_{n}(\\omega) \\neq Y(\\omega) \\}  = 0a$$\n",
        "3. $Y$ is the *limit in probability* of $Y_{n}$ or $Y_{n}$ *converges in probability * to $Y$( denoted as $Y_{n} \\xrightarrow[]{\\text{P}} Y$ or $P - \\lim\\limits_{n \\rightarrow ∞}Y_{n} = Y$), if $$ \\forall \\epsilon > 0, \\quad \\lim_{n \\rightarrow}P \\{ \\omega \\in \\Omega : \\lim_{n \\rightarrow \\infty} Y_{n}(\\omega) \\neq Y(\\omega) \\}  = 0a$$\n",
        "4. $Y$ is the *limit in distribution* of $Y_{n}$ or $Y_{n}$ *converges in distribution* to $Y$( denoted as $Y_{n} \\xrightarrow[]{\\text{D}} Y$, if $$ \\lim_{n \\rightarrow \\infty}F_{Y_{n}}(y) = F_{Y}(y) \\quad \\forall y \\in \\mathbb{R}, F_{Y} \\text{(CDF of Y) is continuous}$$\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "###Relationship between modes of convergence\n",
        "\n",
        "$$Y_{n} \\xrightarrow[]{\\text{pw}} Y \\Rightarrow Y_{n} \\xrightarrow[]{\\text{a.s.}} Y \\Rightarrow Y_{n} \\xrightarrow[]{\\text{P}} Y \\Rightarrow Y_{n} \\xrightarrow[]{\\text{D}} Y$$"
      ],
      "metadata": {
        "id": "VjBb22qGGV7n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Two Laws of Large Numbers\n",
        "\n",
        "Weak Law of Large Numbers\n",
        "---\n",
        "\n",
        "Let $Y_{1}, Y_{3}$ be an iid sequence of random vairables on $(\\Omega , P)$, with finite $\\sigma^{2} = var[Y_{n}] < \\infty$. Let $\\mu = E[Y_{n}]$ Then\n",
        "$\\frac{Y_{1} + Y_{2} + ... + Y_{n}}{n}$ converges in probability to $\\mu$ if\n",
        "\n",
        "$$ [ϵ > 0] \\Rightarrow \\lim_{n \\rightarrow \\infty} P \\Biggl\\{ \\lvert \\frac{1}{n} \\sum_{j =1}^{n} Y_{j} -\\mu \\rvert  > \\epsilon \\Biggl\\} = 0$$\n",
        "\n",
        "\n",
        "###Strong Law of Large Numbers\n",
        "---\n",
        "\n",
        "$$\\lim_{n \\rightarrow \\infty} P \\Biggl\\{  \\frac{1}{n} \\sum_{j=1}^{n} Y_{j} \\neq \\mu \\Biggl\\} =0$$"
      ],
      "metadata": {
        "id": "MTuHPjl852-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Sampling Distribution\n",
        "\n",
        "Let $Y_{1}, Y_{2},..., Y_{n}$ be a random sample of size n from a normal distribution with mean $μ$ and variance $σ^{2}$. Then $\\overline{Y}$ is normally distributwd with mean $\\mu_{\\overline{Y}} = \\mu$ and variance $\\sigma^{2}_{\\overline{Y}} = \\frac{\\sigma^{2}}{n}$, then\n",
        "\n",
        "$$Z = \\frac{\\overline{Y} - \\mu_{\\overline{Y}}}{\\sigma_{\\overline{Y}}}=  \\frac{\\overline{Y} - \\mu }{\\frac{\\sigma}{\\sqrt{n}}}= \\sqrt{n} \\bigg(\\frac{\\overline{Y} - \\mu }{\\sigma}\\bigg)$$\n",
        "\n",
        "\n",
        "\n",
        "Let $Y_{1}, Y_{2},..., Y_{n}$ be a random sample of size n from a normal distribution with mean $μ$ and variance $σ^{2}$. Then $Z_{i} = \\frac{(Yi - μ)}{σ}$ are independent, standard normal random variables, i = 1, 2,..., n, and\n",
        "$$\\sum\\limits^{n}_{i=1} Z^{2}_{i} = \\sum\\limits_{i=1}^{n} \\bigg(\\frac{Y_{i} - μ}{σ}\\bigg)^{2}$$\n",
        "\n",
        "has a $χ^{2}$ distribution with n degrees of freedom (df).\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "Let $Y_{1}, Y_{2},..., Y_{n}$ be a random sample from a normal distribution with mean $μ$ and variance $σ^{2}$. Then\n",
        "\n",
        "$$\\frac{(n - 1)S^{2}}{σ^{2}} = \\frac{1}{σ^{2}} \\sum\\limits^{n}_{i=1} (Y_{i} - \\overline{Y} )^{2}$$\n",
        "\n",
        "has a $χ^{2}$ distribution with $(n − 1)$ df. Also, $Y$ and $S^{2}$ are independent random variables.\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "Let $Z$ be a standard normal random variable and let $W$ be a $χ^{2}$-distributed variable with $ν$ df. Then, if $Z$ and $W$ are independent,\n",
        "\n",
        "$$T = \\frac{Z}{\\sqrt{\\frac{W}{ν}}}$$\n",
        "\n",
        "is said to have a *t-distribution* with $ν$ df.\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "Let $W_{1}$ and $W_{2}$ be independent $χ^{2}$-distributed random variables with $ν_{1}$ and $ν_{2}$ df, respectively. Then\n",
        "\n",
        "$$F = \\frac{ \\frac{W_{1}}{ν_{1}}}{\\frac{W_{2}}{ν_{2}}}$$\n",
        "\n",
        "is said to have an $F$ distribution with $ν_{1}$ numerator degrees of freedom and $ν_{2}$ denominator degrees of freedom.\n",
        "\n"
      ],
      "metadata": {
        "id": "b21P-b9fijJQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Central Limit Theorem\n",
        "---\n",
        "Let $Y_{1}, Y_{2}, ..., Y_{n}$ be iid random variables with $E(Y_{i}) = \\mu$ and $V(Y_{i})= \\sigma^{2} < \\infty$ Define\n",
        "$$U_{n} = \\frac{\\sum\\limits_{i=1}^{n} Y_{i} - n\\mu}{\\sigma \\sqrt{n}}= \\frac{  \\overline{Y } - \\mu }{\\frac{\\sigma}{\\sqrt{n}}}$$\n",
        "<center> where $$\\overline{Y} = \\frac{1}{n} \\sum\\limits_{i=1}^{n}Y_{i}$$</center>\n",
        "\n",
        "Then the distribution ofunction of $U_{n}$ converges to the standard normal distribution function as $n \\rightarrow \\infty$ That is,\n",
        "\n",
        "$$\\lim\\limits_{n \\rightarrow \\infty}P(U_{n} \\leq u) = \\int_{-\\infty}^{u} \\frac{e^{-\\frac{t^{2}}{2}}}{\\sqrt{2\\pi}} \\,dt \\quad \\forall u$$\n",
        "\n"
      ],
      "metadata": {
        "id": "GX6xYOaATx68"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5mKxZqZKggh-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Final Exam Review\n",
        "\n",
        "Math 447- Final Exam\n",
        "- More content on chapter 5+\n",
        "- More time per question\n",
        "- One problem(10 parts) more on what kind of distribution for the function + parameters\n",
        "\n",
        "Topics to review (complicated)\n",
        "* Combinatorics\n",
        "* Conditional Probability - Total Probability - Bayes\n",
        "* gamma(p=\"success\") -hypergeometric(N = 50, n = 24, r = 12)\n",
        "\n",
        "\n",
        "Example\n",
        " If N=50, (12 = green, 38 = red), sample of 24 = n.\n",
        " --> Probability Distribution Hypergeo~(N=50, n=24, R=12)\n",
        "\n",
        " Central Limit Theorem\n",
        "\n",
        "* Random sampling action (RSA)\n",
        "\n",
        "*  $$E[\\overline{Y_{n}} = E\\Bigg[\\frac{\\sum\\limits_{j=1}^{n}y_{j}}{n} \\Bigg] = \\frac{1}{n} \\sum\\limits_{j=1}^{n}E[Y_{j}] = \\frac{n}{n} \\mu$$\n",
        "\n",
        "* $$Var[\\overline{Y_{n}}] = Var\\Bigg[\\frac{\\sum\\limits_{j=1}^{n}y_{j}}{n} \\Bigg] = \\frac{1}{n^{2}} \\sum\\limits_{j=1}^{n}Var[Y_{j}] = \\frac{n}{n^{2}} \\sigma^{2} = \\frac{\\sigma^{2}}{n}  $$\n",
        "\n",
        "* for large enough n\n",
        "distribution of sample mean\n",
        "$\\frac{\\overline{Y_{n}} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} \\xrightarrow{D} N(0,1)$\n",
        "\n",
        "* distribution of sum of y- $\\sum\\limits_{j=1}^{n}Y_{j} \\sim N(n\\mu , n \\sigma^{2})$\n",
        "\n",
        "\n",
        "\n",
        "Poisson Distr. (Another approximation)\n",
        " - $λ (=/≈) np$\n",
        " -\n",
        "\n",
        "---\n",
        "\n",
        "Question:\n",
        "How many different license plate (7 (4 letters + 3 numbers))?\n",
        "\n",
        "$ 26^{4} * 10^{3}$\n",
        "\n",
        "Duplicates for letter/ No duplicate for numbers\n",
        "\n",
        "$26^{4} (10*9*8 )$\n",
        "\n",
        "$P^{n}_{r} = \\frac{n!}{(n-r)!}$\n",
        "\n",
        "Combinations =$\\binom{26}{4} \\binom{10}{7}$\n",
        "\n",
        "---\n",
        "\n",
        "Question: What's the distr. of $m_{y}(t) = e^{ -6t + 8 t^{2}}$\n",
        "Answer: $N(-6, 16)$\n",
        "\n",
        "---\n",
        "What's the expected value of $E[3Y^{2}]$\n",
        "Answer: $3 E[Y^{2}] = 3\\int_{-\\infty}^{\\infty}y^{2} f(y) \\,dy$\n",
        "\n",
        "---\n",
        "\n",
        "$expon(\\beta) = gamma(1, \\beta)$\n",
        "\n",
        "$\\chi(v) = gamma(\\frac{v}{2}, 2)$\n",
        "\n",
        "---\n",
        "\n",
        "WMS Example 6.1\n"
      ],
      "metadata": {
        "id": "JyTT6X_9R1UB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Covariance"
      ],
      "metadata": {
        "id": "AWirzupUurBN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example"
      ],
      "metadata": {
        "id": "G0LRGgFVxi5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "$u  = h(y) \\quad u = y^{2} $\n",
        "\n",
        "$F_{u}(u) = P\\{ U \\leq u\\} = \\{ y^{2} \\leq u\\} = \\{  -\\sqrt{u} \\leq y \\leq \\sqrt{u}\\}$\n",
        "\n",
        "$= F_{y}(\\sqrt{u}) - F_{y}(\\sqrt{-u}) \\$\n",
        "\n",
        "$⇒ f_{u}(u) = \\frac{1}{2} u^{-\\frac{1}{2}}f_{y}(\\sqrt{u}) = \\frac{1}{2\\sqrt{u}}(f_{y}(u) + f_{y}(\\sqrt{u}))$\n"
      ],
      "metadata": {
        "id": "G1M9y0bpu79t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Example\n",
        "$U = y_{1} + y_{2} \\quad u=h(y_{1}, y_{2}) = y_{1} + y_{2}$\n",
        "\n",
        "$B:= \\{ (y_{1}, y_{2}): 0 <  1 - y_{1} < 1 \\} $\n",
        "\n",
        "$y_{1}, y_{2}$ ~  unifnorm on $B$\n",
        "\n",
        "$$\n",
        "f_{y_{1}, y_{2} } =\n",
        "\\begin{cases}\n",
        "2 & \\text{on } B \\\\\n",
        "0 & \\text{else  } x \\leq 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "$$\n",
        "F_{U}(u) =\n",
        "\\begin{cases}\n",
        "0 & \\text{if } u \\leq 0 \\\\\n",
        "?? & \\text{if  } x \\leq 0  \\\\\n",
        "1 & \\text{if  } u > 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "$0 < u <  1 ⇒ F_{U}(u) = \\iint\\limits_{ \\{ y_{1} + y_{2} \\leq u \\} ∩ B }(f_{\\vec(y)} \\\\d\\vec{y}) = \\int_0^u\\int_0^{u-y_{1}}(2)\\;dy_{2}\\;dy_{1}\n",
        "$\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "3VDEzy0nxDcE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example"
      ],
      "metadata": {
        "id": "DFikJCWl38rH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$U = h(y)$\n",
        "\n",
        "$\n",
        "u=h_{y} =\n",
        "\\begin{cases}\n",
        "y & \\text{if } y \\leq 0 \\\\\n",
        "e^{-y} & \\text{if  } y > 0\n",
        "\\end{cases}\n",
        "$\n",
        "---\n",
        "$y \\leq 0 ⇔ u \\leq 0\n"
      ],
      "metadata": {
        "id": "m9xEqL5q3_Hj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$f_{U}(u) = f_{y}(h^{-1}(u)) \\lvert \\frac{dh^{-1}(u)}{du} \\rvert  \\quad \\vec{u} = \\vec{h}(\\vec{y}) $\n",
        "\n",
        "$f_{\\vec{y}}(\\vec{h}^{-1}(\\vec{u})) \\cdot \\lvert \\frac{dh^{-1}(\\vec{u})}{d\\vec{u}}(\\vec{u}) \\rvert $\n",
        "$$\n",
        "= f_{\\vec{u}}(\\vec{u}) = f_{\\vec{y}}(h^{-1}(u)) \\lvert \\mathscr{F} \\rvert\n",
        "$$"
      ],
      "metadata": {
        "id": "0bb8PJ_04loh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test"
      ],
      "metadata": {
        "id": "DkXhzOR3u3hh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Order Statistic"
      ],
      "metadata": {
        "id": "44clZZffPKj_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Order Statistic\n",
        "For $y \\in \\mathbb{R}$, the CDF of the $k^{th}$ order statistic  $(k = 1,..., n)$ satisfies the following\n",
        "\n",
        "1. $$F_{Y_{1} } (y) = 1 - [1- F(y)]^{n}$$\n",
        "2. $$F_{Y_{n} } (y) = [F(y)]^{n}$$\n",
        "3. $$F_{Y_{k} } (y) = 1- \\sum\\limits_{j=0}^{k-1}\\binom{n}{h}[F(y)]^{j}[1-F(y)]^{n-j}$$\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "fc2QlQgCPOhA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Chapter 7 - Sampling Distributions and the Central Limit Theorem\n",
        "* Sampling Distributions Related to the Normal Distribution\n",
        "* The Central Limit Theorem\n",
        "* The Normal Approximation to the Binomial"
      ],
      "metadata": {
        "id": "lqIKVfrgeA3c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Limits for Sequences of Random Variables"
      ],
      "metadata": {
        "id": "xtKrBk5pRqiI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $Y_{n} (n \\in \\mathbb{N})$ and $Y$ be random variables on a probability space $(Ω,P)$ We define ...\n",
        "\n",
        "####Pointwise Limit\n",
        "1. $$Y_{n} \\xrightarrow[]{\\text{pw}} Y $$ or $$pw - \\lim_{n \\rightarrow ∞} $$\n"
      ],
      "metadata": {
        "id": "Bo2rDWntRy84"
      }
    }
  ]
}